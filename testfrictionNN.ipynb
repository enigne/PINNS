{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307f4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from utils import *\n",
    "from equations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f887ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochADAM = 100000\n",
    "epochLBFGS=50000\n",
    "N_u=50\n",
    "N_f=100\n",
    "seed=1234\n",
    "log_frequency=1000\n",
    "history_frequency=10\n",
    "NLayers=8\n",
    "NNeurons=20\n",
    "noiseLevel=[]\n",
    "weights = [5,3,5,6,14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025417ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = {}\n",
    "# Data size on the solution u\n",
    "hp[\"N_u\"] = N_u\n",
    "# Collocation points size, where weâ€™ll check for f = 0\n",
    "hp[\"N_f\"] = N_f\n",
    "# DeepNN topology (1-sized input [x], NLayers hidden layer of NNeurons-width, 1-sized output [u]\n",
    "hp[\"layers\"] = [1]+[NNeurons]*NLayers+[1]\n",
    "# DeepNN topology (1-sized input [x], NLayers hidden layer of NNeurons-width, 2-sized output [h, H]\n",
    "hp[\"h_layers\"] = [1]+[NNeurons]*NLayers+[2]\n",
    "# DeepNN topology (1-sized input [x], NLayers hidden layer of NNeurons-width, 1-sized output [C]\n",
    "hp[\"C_layers\"] = [1]+[NNeurons]*NLayers+[1]\n",
    "# DeepNN topology (1-sized input [x], NLayers hidden layer of NNeurons-width, 4-sized input [u,H,h,c], \n",
    "#   1-sized output [taub]\n",
    "hp[\"friction_layers\"] = [4]+[NNeurons]*8+[1]\n",
    "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
    "hp[\"tf_epochs\"] = epochADAM\n",
    "hp[\"tf_lr\"] = 0.001\n",
    "hp[\"tf_b1\"] = 0.99\n",
    "hp[\"tf_eps\"] = 1e-1\n",
    "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
    "hp[\"nt_epochs\"] = epochLBFGS\n",
    "hp[\"log_frequency\"] = log_frequency\n",
    "# Record the history\n",
    "hp[\"save_history\"] = True\n",
    "hp[\"history_frequency\"] = history_frequency\n",
    "# path for loading data and saving models\n",
    "repoPath = \"./\"\n",
    "appDataPath = os.path.join(repoPath, \"matlab_SSA\", \"DATA\")\n",
    "path = os.path.join(appDataPath, \"Helheim_Weertman_iT080_PINN_flowline_CF_2dInv.mat\")\n",
    "\n",
    "loss_weights = [10**(-w) for w in weights]\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "# check the input\n",
    "if type(noiseLevel) != list:\n",
    "    noiseLevel = [] # set to no noise\n",
    "\n",
    "if noiseLevel:\n",
    "    modelPath = \"./Models/SSA1D_3NN_\"+str(NLayers)+\"x\"+str(NNeurons)+\"_noise_\" + \"\".join([str(i)+\"_\" for i in noiseLevel])+ \"weights\" + \"\".join([str(w)+\"_\" for w in weights]) + now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "else:\n",
    "    modelPath = \"./Models/SSA1D_3NN_\"+str(NLayers)+\"x\"+str(NNeurons)+\"_weights\"+ \"\".join([str(w)+\"_\" for w in weights]) + now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "modelPath += (\"_seed_\" + str(seed) if seed else \"\")\n",
    "# + \"ADAM\"+str(hp[\"tf_epochs\"]) +\"_BFGS\"+str(hp[\"nt_epochs\"])\n",
    "reloadModel = False # reload from previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15d4a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, Exact_vel, X_star, u_star, X_u_train, u_train, X_f, X_bc, u_bc, X_cf, n_cf, xub, xlb, uub, ulb, mu = prep_Helheim_data_flowline(path, hp[\"N_u\"], hp[\"N_f\"]) #}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe7371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(hp)\n",
    "pinn = SSA1D_frictionNN(hp, logger, X_f,\n",
    "        X_bc, u_bc,\n",
    "        X_cf, n_cf,\n",
    "        xub, xlb, uub, ulb,\n",
    "        modelPath, reloadModel,\n",
    "        mu=mu,\n",
    "        loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13216a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # error function for logger\n",
    "    X_u = pinn.tensor(X_star)\n",
    "    u = pinn.tensor(u_star)\n",
    "    def error():\n",
    "        return pinn.test_error(X_u, u)\n",
    "    logger.set_error_fn(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6073e4a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    pinn.fit(X_u_train, u_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae127e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
